---
title: "hw5_vy2196"
output: github_document
---

```{r}
library(tidyverse)
library(rvest)

```

### Problem 1

```{r}
# Function to check if at least two people share a birthday
share_birthday = function(n) {
  birthdays = sample(1:365, n, replace = TRUE)  # randomly assign birthdays
  return(length(unique(birthdays)) < n)          
}

set.seed(1)
n_sim = 10000 # number of simulations per group size

group_sizes = 2:50
prob_shared = numeric(length(group_sizes))

for (i in seq_along(group_sizes)) {
  n = group_sizes[i]
  results = replicate(n_sim, share_birthday(n))
  prob_shared[i] = mean(results)   # proportion of TRUE results
}
```

```{r}
#making a plot
sim_results = tibble(
  n = 2:50,
  prob = map_dbl(n, ~ mean(replicate(10000, share_birthday(.x))))
)

ggplot(sim_results, aes(x = n, y = prob)) +
  geom_line(color = "blue") +
  geom_point(color = "darkblue") +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  labs(title = "Probability of Shared Birthday vs Group Size",
       x = "Group Size",
       y = "Probability of Shared Birthday") +
  theme_minimal()
```
The probability is around 50% when the group size is about 23 people, and it reaches 100% when the group size is about 50 people.

### Problem 2

```{r}
library(broom)

n = 30
sigma = 5
mu_values = 0:6
n_sim = 5000

simulate_ttest = function(mu, n = 30, sigma = 5) {
  x = rnorm(n, mean = mu, sd = sigma)
  t_res = t.test(x, mu = 0)
  
  # Clean the t-test output using broom::tidy
  broom::tidy(t_res) |>
    select(estimate, p.value) |>  # keep only needed columns
    mutate(true_mu = mu)}


# Run simulations for each mu
sim_results = map_df(mu_values, function(mu) {
  replicate(n_sim, simulate_ttest(mu, n, sigma), simplify = FALSE) |>
    bind_rows()})

```

```{r}
#Compute power 
power_results = sim_results |>
  group_by(true_mu) |>
  summarise(power = mean(p.value < 0.05))

# Plot Power vs Effect Size 
ggplot(power_results, aes(x = true_mu, y = power)) +
  geom_line(color = "blue", size = 1) +
  geom_point(size = 2) +
  labs(
    title = "Power of One-Sample t-test vs Effect Size (Î¼)",
    x = expression(True~mean~mu),
    y = "Power (P(reject H_0))"
  ) +
  theme_minimal()

```
As the true mean increases, the power of the test also increases. This means when the true mean is close to 0, the test rarely rejects null hypothesis. As the true mean increases, the difference between the sample mean and the hypothesized mean becomes easier to detect, so the test rejects the null hypothesis more often.

```{r}
# Average estimated Î¼ 
mu_summary <- sim_results |>
  group_by(true_mu) |>
  summarise(
    avg_estimate_all = mean(estimate),
    avg_estimate_rejected = mean(estimate[p.value < 0.05])
  )

# Plot Average Estimates 
ggplot(mu_summary, aes(x = true_mu)) +
  geom_line(aes(y = avg_estimate_all), color = "black", size = 1.2) +
  geom_point(aes(y = avg_estimate_all), color = "black", size = 2) +
  geom_line(aes(y = avg_estimate_rejected), color = "red", size = 1.2, linetype = "dashed") +
  geom_point(aes(y = avg_estimate_rejected), color = "red", size = 2) +
  geom_abline(slope = 1, intercept = 0, color = "gray50", linetype = "dotted") +
  labs(
    title = "Average Estimated Mean vs True Mean (Î¼)",
    subtitle = "Black = all samples | Red = only rejected samples",
    x = expression(True~mean~mu),
    y = expression(Average~hat(mu))
  ) +
  theme_minimal()
```

The sample average of ðœ‡Ì‚  across tests for which the null is rejected approximately equal to the average estimate of ðœ‡Ì‚ when the true mean is greater than or equal to 4. The red line peaks at 1 because that's where the most extreme 5% of all the samples are, which shows extreme bias. And it slowly merges with the black line when true mean is 4 or more since more and more samples are significant


### Problem 3

```{r}
library(purrr)
```

```{r}
homicides = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")

glimpse(homicides)

```
The raw data includes `uid` which is specific case id for specific city, information about the victim such as first and last name, gender, city, state, and the disposition. 

```{r}
city_summary = homicides |>
  mutate(city_state = paste(city, state, sep = ", ")) |>
  group_by(city_state) |>
  summarise(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))

```

```{r}
baltimore = city_summary |>
  filter(city_state == "Baltimore, MD")

baltimore_test = prop.test(
  x = baltimore$unsolved,
  n = baltimore$total
)

baltimore_tidy = tidy(baltimore_test)
baltimore_tidy |>
  select(estimate, conf.low, conf.high)
```
```{r}
city_results = city_summary |>
  mutate(
    test = map2(unsolved, total, ~ prop.test(.x, .y)),
    tidy = map(test, tidy)) |>
  unnest(tidy) |>
  select(city_state, estimate, conf.low, conf.high)

city_results |>
  mutate(city_state = fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = estimate, y = city_state)) +
  geom_point() +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high), width = 0.2) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "Proportion Unsolved (with 95% CI)",
    y = "City"
  ) +
  theme_minimal()


```

